{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "defining a base class for layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from Activation_Functions.Layer import Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#linear layer \n",
    "class Linear(Layer):\n",
    "    def __init__ (self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.w = 0.1 * np.random.rand(input_dim, output_dim)\n",
    "        self.b = np.zeros((1, output_dim))\n",
    "        self.dw = np.zeros_like(self.w)\n",
    "        self.db = np.zeros_like(self.b)\n",
    "    def forward(self, input):\n",
    "        self.input  = input\n",
    "        self.output= np.dot(input, self.w) + self.b\n",
    "        return self.output\n",
    "    def backward(self, up_gradient: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Backpropagate the gradients through this layer.\"\"\"\n",
    "        #dw = dL/dw\n",
    "        self.dw = np.dot(self.input.T, up_gradient)  \n",
    "        self.db = np.sum(up_gradient, axis=0, keepdims=True)\n",
    "        down_grad = np.dot(up_gradient, self.w.T)\n",
    "        return down_grad\n",
    "    def step(self, learning_rate: float) -> None:\n",
    "        \"\"\"Update the weights and biases using the gradients.\"\"\"\n",
    "        self.w -= learning_rate * self.dw\n",
    "        self.b -= learning_rate * self.db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      " [[1. 2. 3.]]\n",
      "Weights:\n",
      " [[0.05488135 0.07151894]\n",
      " [0.06027634 0.05448832]\n",
      " [0.04236548 0.06458941]]\n",
      "Biases:\n",
      " [[0. 0.]]\n",
      "Output:\n",
      " [[0.30253047 0.37426381]]\n"
     ]
    }
   ],
   "source": [
    "#testing the linearilty\n",
    "np.random.seed(0)  \n",
    "linear_layer = Linear(3, 2)\n",
    "input = np.array([[1.0, 2.0, 3.0]]) \n",
    "output = linear_layer(input)\n",
    "print(\"Input:\\n\", input)\n",
    "print(\"Weights:\\n\", linear_layer.w)\n",
    "print(\"Biases:\\n\", linear_layer.b)\n",
    "print(\"Output:\\n\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerPerceptron:\n",
    "    def __init__(self, layers, loss_function, learning_rate):\n",
    "        self.layers = layers\n",
    "        self.loss_function = loss_function\n",
    "        self.learning_rate = learning_rate\n",
    "    def __call__(self, input):\n",
    "        #forward passing the model\n",
    "        return self.forward(input)\n",
    "    def forward (self, input):\n",
    "        for layer in self.layers:\n",
    "            input = layer.forward(input)\n",
    "        return input\n",
    "    def loss(self, prediction, target):\n",
    "        return self.loss_function(prediction, target)\n",
    "    def backward(self):\n",
    "        up_gradient = self.loss_function.backward()\n",
    "        for layer in reversed(self.layers):\n",
    "            up_gradient = layer.backward(up_gradient)\n",
    "    def update(self):\n",
    "        for layer in self.layers:\n",
    "            layer.step(self.learning_rate)\n",
    "    def train(self, x_train, y_train, epochs, batch_size):\n",
    "        losses = np.empty(epochs)\n",
    "        for epoch in ((epochs)):\n",
    "            running_loss = 0.0\n",
    "            for i in range(0, len(x_train), batch_size):\n",
    "                x_batch = x_train[i:i + batch_size]\n",
    "                y_batch = y_train[i:i + batch_size]\n",
    "                prediction = self.forward(x_batch)\n",
    "                running_loss += self.loss(prediction, y_batch) * batch_size\n",
    "                self.backward()\n",
    "                self.update()\n",
    "            running_loss /= len(x_train)\n",
    "            print(f'loss is equal to {running_loss: .2f}')\n",
    "            losses[epoch] = running_loss\n",
    "        return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total dataset: 100 samples\n",
      "Training set: 70 samples\n",
      "Test set: 30 samples\n",
      "\n",
      "First 5 training samples (X_train, y_train):\n",
      "Features: [ 1.63891973 70.51914615], Target: 0\n",
      "Features: [ 6.01676983 61.39675813], Target: 0\n",
      "Features: [ 2.71593972 62.46461146], Target: 0\n",
      "Features: [ 2.07439325 85.15094794], Target: 0\n",
      "Features: [ 4.42080554 62.89708139], Target: 0\n",
      "\n",
      "First 5 test samples (X_test, y_test):\n",
      "Features: [ 1.82005708 52.03875708], Target: 0\n",
      "Features: [10.75934017 76.48252892], Target: 1\n",
      "Features: [ 4.92428659 76.7887342 ], Target: 0\n",
      "Features: [ 5.27545019 68.38915664], Target: 0\n",
      "Features: [ 1.27961039 94.35432121], Target: 0\n",
      "\n",
      "Training set statistics:\n",
      "Pass rate: 0.53\n",
      "\n",
      "Test set statistics:\n",
      "Pass rate: 0.47\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate synthetic dataset\n",
    "n_samples = 100\n",
    "\n",
    "# Feature 1: Study hours per week (between 1 and 12)\n",
    "study_hours = np.random.uniform(1, 12, n_samples)\n",
    "\n",
    "# Feature 2: Previous test score (between 50 and 100)\n",
    "previous_score = np.random.uniform(50, 100, n_samples)\n",
    "\n",
    "# Creating a formula that combines both features to determine probability of passing\n",
    "# Higher study hours and previous scores increase chance of passing\n",
    "probability = 1 / (1 + np.exp(-(0.7 * study_hours + 0.05 * previous_score - 8)))\n",
    "\n",
    "# Generate binary outcome (pass/fail) based on probability\n",
    "# 1 = pass, 0 = fail\n",
    "passed = np.random.binomial(1, probability)\n",
    "\n",
    "# Create feature matrix X and target vector y\n",
    "X = np.column_stack((study_hours, previous_score))\n",
    "y = passed\n",
    "\n",
    "# Split data into training (70%) and testing (30%) sets\n",
    "# First, create indices for the split\n",
    "n_train = int(0.7 * n_samples)\n",
    "indices = np.random.permutation(n_samples)\n",
    "train_idx, test_idx = indices[:n_train], indices[n_train:]\n",
    "\n",
    "# Create train/test splits\n",
    "X_train = X[train_idx]\n",
    "y_train = y[train_idx]\n",
    "X_test = X[test_idx]\n",
    "y_test = y[test_idx]\n",
    "\n",
    "# Print dataset sizes\n",
    "print(f\"Total dataset: {n_samples} samples\")\n",
    "print(f\"Training set: {len(X_train)} samples\")\n",
    "print(f\"Test set: {len(X_test)} samples\")\n",
    "\n",
    "# Print first 5 samples from training set\n",
    "print(\"\\nFirst 5 training samples (X_train, y_train):\")\n",
    "for i in range(min(5, len(X_train))):\n",
    "    print(f\"Features: {X_train[i]}, Target: {y_train[i]}\")\n",
    "\n",
    "# Print first 5 samples from test set\n",
    "print(\"\\nFirst 5 test samples (X_test, y_test):\")\n",
    "for i in range(min(5, len(X_test))):\n",
    "    print(f\"Features: {X_test[i]}, Target: {y_test[i]}\")\n",
    "\n",
    "# Basic statistics for train/test sets\n",
    "print(\"\\nTraining set statistics:\")\n",
    "print(f\"Pass rate: {np.mean(y_train):.2f}\")\n",
    "print(\"\\nTest set statistics:\")\n",
    "print(f\"Pass rate: {np.mean(y_test):.2f}\")\n",
    "\n",
    "# Optionally save to NumPy files\n",
    "# np.save('X_train_logistic.npy', X_train)\n",
    "# np.save('y_train_logistic.npy', y_train)\n",
    "# np.save('X_test_logistic.npy', X_test)\n",
    "# np.save('y_test_logistic.npy', y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'MSE' from 'Loss_Functions.MSE' (/home/majid/Desktop/WebDev/Machine_Learning_From_Scratch/NN/../Loss_Functions/MSE.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[82]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m X_train.shape\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mActivation_Functions\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mSigmoid\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Sigmoid\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mLoss_Functions\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mMSE\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MSE\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'MSE' from 'Loss_Functions.MSE' (/home/majid/Desktop/WebDev/Machine_Learning_From_Scratch/NN/../Loss_Functions/MSE.py)"
     ]
    }
   ],
   "source": [
    "X_train.shape\n",
    "from Activation_Functions.Sigmoid import Sigmoid\n",
    "from Loss_Functions.MSE import MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers=[Linear(40, 2), Sigmoid()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MSE' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[81]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m model = MultiLayerPerceptron(layers, \u001b[43mMSE\u001b[49m(), learning_rate=\u001b[32m0.01\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'MSE' is not defined"
     ]
    }
   ],
   "source": [
    "model = MultiLayerPerceptron(layers, MSE(), learning_rate=0.01)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
